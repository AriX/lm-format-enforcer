{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Format Enforcer Integration with ExLlamaV2\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_exllamav2_integration.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook shows how you can integrate with the [ExLlamaV2](https://github.com/turboderp/exllamav2/) library. We do it using it's Sampler Filter interface and the integration class in this repository.\n",
    "\n",
    "ExLlamaV2 is one of the fastest inference engines, but does not support any of the popular constrained decoding libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "\n",
    "We begin by installing the dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install exllamav2 lm-format-enforcer huggingface-hub\n",
    "\n",
    "# When running from source / developing the library, use this instead\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "## os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "This demo uses [Llama2 bpw weights by turboderp](https://huggingface.co/turboderp/Llama2-7B-exl2/tree/8.0bpw). We will use huggingface hub to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamgat/mambaforge/envs/commentranker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 217885.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_directory = snapshot_download(repo_id=\"turboderp/Llama2-7B-exl2\", revision=\"6463dd96f3694a87b777852f8bd979dbaeb2b839\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing ExLlamaV2\n",
    "\n",
    "We follow the [inference.py example](https://github.com/turboderp/exllamav2/blob/master/examples/inference.py) from the ExLlamaV2 repo. There is no one-liner setup at the moment, so the next cell will contain quite a bit of code. It is all from the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /mnt/wsl/PHYSICALDRIVE1p3/huggingface/hub/models--turboderp--Llama2-7B-exl2/snapshots/6463dd96f3694a87b777852f8bd979dbaeb2b839\n"
     ]
    }
   ],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "# Initialize model and cache\n",
    "\n",
    "config = ExLlamaV2Config()\n",
    "config.model_dir = model_directory\n",
    "config.prepare()\n",
    "\n",
    "model = ExLlamaV2(config)\n",
    "print(\"Loading model: \" + model_directory)\n",
    "\n",
    "cache = ExLlamaV2Cache(model, lazy = True)\n",
    "model.load_autosplit(cache)\n",
    "\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Prepare settings\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
    "\n",
    "max_new_tokens = 150\n",
    "\n",
    "generator.warmup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell executed successfully, you have propertly set up your Colab runtime and loaded the ExLlamaV2 model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions to make display nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text with the LM Format Enforcer Logits Processor\n",
    "\n",
    "ExLlamaV2's `Sampler.Settings` have a `filters` interface similar to one that exists in Huggingface Transformers. We will connect to this API and filter the forbidden logits.\n",
    "\n",
    "The integration class `ExLlamaV2TokenEnforcerFilter` does just that. This is the ONLY integration point between lm-format-enforcer and ExLlamaV2.\n",
    "\n",
    "Note that in this notebook we use `generate_simple()`, but the integration works with all ExLlamaV2 generation methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmformatenforcer.characterlevelparser import CharacterLevelParser\n",
    "from lmformatenforcer.integrations.exllamav2 import ExLlamaV2TokenEnforcerFilter\n",
    "from typing import Optional\n",
    "\n",
    "def exllamav2_with_format_enforcer(prompt: str, parser: Optional[CharacterLevelParser] = None) -> str:\n",
    "    if parser is None:\n",
    "        settings.filters = []\n",
    "    else:\n",
    "        settings.filters = [ExLlamaV2TokenEnforcerFilter(parser, tokenizer)]\n",
    "    result = generator.generate_simple(prompt, settings, max_new_tokens, seed = 1234)\n",
    "    return result[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExLlamaV2 + JSON Use case\n",
    "\n",
    "Now we demonstrate using ```JsonSchemaParser```. We create a pydantic model, generate the schema from it, and use that to enforce the format.\n",
    "The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"title\": \"AnswerFormat\", \"type\": \"object\", \"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"]}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, Without json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "\n",
       "The JSON schema that you need to provide is in this format, but with your own data and the correct name of the person. For example, if I wanted the answer for Michael Jordan, my response would look like this: { 'first-name': 'Michael', 'last-name': 'Jordan', 'year-of-birth': 1963, 'num-seasons-in-nba': 5 }. The first line contains the title (which can be anything) followed by a colon (:). Afterwards are three properties. Each property has its own key value pair as well as an associated schema that describes what type of object it should contain. Finally there's one required element per\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "\n",
       "\n",
       "    {\n",
       "        \"first_name\": \"Michael\",\n",
       "        \"last_name\": \"Jordan\",\n",
       "        \"year_of_birth\": 1963,\n",
       "        \"num_seasons_in_nba\": 15\n",
       "    }\n",
       "\n",
       "   \n",
       "\n",
       "   \n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json mode (json output, no specific schema) enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "\n",
       "\n",
       "    [[\"Michael Jordan\"],\n",
       "     [\"1963-02-17\", 45, 15],\n",
       "     [\"Charlotte Hornets\", 8, 15]]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "prompt = question_with_schema\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result = exllamav2_with_format_enforcer(prompt, parser=None)\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "parser = JsonSchemaParser(AnswerFormat.schema())\n",
    "result = exllamav2_with_format_enforcer(prompt, parser=parser)\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, With json mode (json output, no specific schema) enforcing:\")\n",
    "parser = JsonSchemaParser(None)\n",
    "result = exllamav2_with_format_enforcer(prompt, parser=parser)\n",
    "display_content(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the enforced output matches the required schema, while the unenforced does not. We have successfully integrated with ExLlamaV2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExLlamaV2 + Regular Expressions Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Q: When was Michael Jordan Born? Please answer in mm/dd/yyyy format. A:\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Without format forcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       " The birthday of basketball legend, Michael Jordan is 17th February 1963\n",
       "Q: What was the first name of his father? A: James R. Jordan Sr.\n",
       "Q: In which city did he complete both high school and college? A: Wilmington, North Carolina\n",
       "Q: Who were the Chicago Bulls head coaches during his playing career with them? A: Phil Jackson (1984-1998) and Doug Collins (1998–2002).\n",
       "Q: How many times did he win an NBA championship while a member of the Chicago Bulls? A: Six times. He won three consecutive championships from 1991 to\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**With regex force. Regex: ``` In mm/dd/yyyy format, Michael Jordan was born in (0?[1-9]|1[0-2])\\/(0?[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2}```**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  In mm/dd/yyyy format, Michael Jordan was born in 2/17/1963\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lmformatenforcer import RegexParser\n",
    "\n",
    "date_regex = r'(0?[1-9]|1[0-2])\\/(0?[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2}'\n",
    "answer_regex = ' In mm/dd/yyyy format, Michael Jordan was born in ' + date_regex\n",
    "question = 'Q: When was Michael Jordan Born? Please answer in mm/dd/yyyy format. A:'\n",
    "prompt = question\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "\n",
    "display_header(\"Without format forcing:\")\n",
    "result = exllamav2_with_format_enforcer(prompt, parser=None)\n",
    "display_content(result)\n",
    "\n",
    "\n",
    "display_header(f\"With regex force. Regex: ```{answer_regex}```\")\n",
    "parser = RegexParser(answer_regex)\n",
    "result = exllamav2_with_format_enforcer(prompt, parser=parser)\n",
    "display_content(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with regex forcing enabled, we got a valid output. Without it, we did not get it in the structure that we wanted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmformatenforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
