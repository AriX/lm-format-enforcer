{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This colab-friendly notebook is targeted at demoing the enforcer on LLAMA2. It is able to be executed on a free GPU on Google Colab.\n",
    "Make sure that your runtime is set to GPU (top right corner arrow -> change runtime type -> GPU - T4 at the time of creating this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the dependencies. This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an API key, and insert it when executing the next cell will request it. Links:\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamgat/mambaforge/envs/lmformatenforcer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch lm-format-enforcer huggingface_hub accelerate bitsandbytes cpm_kernels\n",
    "!huggingface-cli login\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 12.29s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "device = 'cuda'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.pretraining_tp = 1\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    raise Exception('GPU not available')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell executed successfully, you have propertly set up your Colab runtime and huggingface account!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the prompting style according to the demo at https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/app.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message: str, chat_history: list[tuple[str, str]],\n",
    "               system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    for user_input, response in chat_history:\n",
    "        user_input = user_input.strip() if do_strip else user_input\n",
    "        do_strip = True\n",
    "        texts.append(f'{user_input} [/INST] {response.strip()} </s><s>[INST] ')\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)\n",
    "\n",
    "\n",
    "def get_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> int:\n",
    "    prompt = get_prompt(message, chat_history, system_prompt)\n",
    "    input_ids = tokenizer([prompt], return_tensors='np', add_special_tokens=False)['input_ids']\n",
    "    return input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function is fairly straightforward, except for the optional parameters ```required_regex``` / ```required_str``` / ```required_json_schema``` which activate the appropriate ```CharacterLevelParser``` to be used by the format enforcer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lmformatenforcer import JsonSchemaParser, CharacterLevelParser, RegexParser, StringParser, generate_enforced\n",
    "\n",
    "\n",
    "def run(message: str,\n",
    "        chat_history: list[tuple[str, str]],\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 50,\n",
    "        required_regex: str = None,\n",
    "        required_str: str = None,\n",
    "        required_json_schema: dict = None) -> str:\n",
    "    prompt = get_prompt(message, chat_history, system_prompt)\n",
    "    inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False, return_token_type_ids=False).to(device)\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        inputs,\n",
    "        # streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature,\n",
    "        num_beams=1,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    parser: CharacterLevelParser = None\n",
    "    if required_regex:\n",
    "        parser = RegexParser(required_regex)\n",
    "    if required_str:\n",
    "        parser = StringParser(required_str)\n",
    "    if required_json_schema:\n",
    "        parser = JsonSchemaParser(required_json_schema)\n",
    "\n",
    "    if parser:\n",
    "        output = generate_enforced(model, tokenizer, parser, **generate_kwargs)\n",
    "    else:\n",
    "        output = model.generate(**generate_kwargs)\n",
    "\n",
    "    sequence = output.sequences[0]\n",
    "    string_output = tokenizer.decode(sequence, skip_special_tokens=True, skip_prompt=True)\n",
    "    if parser:\n",
    "        enforced_scores_dict = output.enforced_scores\n",
    "        enforced_scores = pd.DataFrame(enforced_scores_dict)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        pd.set_option('display.max_columns', 10)\n",
    "        pd.set_option('display.max_rows', 200)\n",
    "        print(enforced_scores)\n",
    "    return string_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples for the simple (non-json) case. Note that the ```required_regex``` does not support the full regex syntax, see the README for full limiations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without enforcing\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix. [/INST]  Sure, I'd be happy to help! Michael Jordan was born in 1963.\n",
      "With regex force. Regex:  Michael Jordan was Born in (\\d)+.\n",
      "   generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
      "0                ▁                29871         1.000000             ▁              29871       1.000000\n",
      "1          Michael                24083         0.000027         ▁Sure              18585       0.959473\n",
      "2          ▁Jordan                18284         1.000000       ▁Jordan              18284       1.000000\n",
      "3             ▁was                  471         1.000000          ▁was                471       1.000000\n",
      "4            ▁Born                19298         0.000008         ▁born               6345       1.000000\n",
      "5              ▁in                  297         0.994629           ▁in                297       0.994629\n",
      "6                ▁                29871         0.982422             ▁              29871       0.982422\n",
      "7                1                29896         1.000000             1              29896       1.000000\n",
      "8                9                29929         1.000000             9              29929       1.000000\n",
      "9                6                29953         1.000000             6              29953       1.000000\n",
      "10               3                29941         1.000000             3              29941       1.000000\n",
      "11               .                29889         0.999512             .              29889       0.999512\n",
      "12            </s>                    2         0.981445          </s>                  2       0.981445\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix. [/INST] Michael Jordan was Born in 1963.\n",
      "With string force\n",
      "  generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
      "0               T                29911         0.000000             ▁              29871       1.000000\n",
      "1              he                  354         0.000050             ▁              29871       0.105652\n",
      "2         ▁answer                 1234         0.076416         ▁year               1629       0.522461\n",
      "3             ▁is                  338         0.059998           ▁to                304       0.938965\n",
      "4               ▁                29871         0.131104             :              29901       0.868652\n",
      "5               1                29896         1.000000             1              29896       1.000000\n",
      "6               9                29929         1.000000             9              29929       1.000000\n",
      "7               6                29953         1.000000             6              29953       1.000000\n",
      "8               3                29941         1.000000             3              29941       1.000000\n",
      "9            </s>                    2         0.000646             .              29889       0.999023\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix. [/INST]The answer is 1963\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "MAX_MAX_NEW_TOKENS = 200\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "MAX_INPUT_TOKEN_LENGTH = 4000\n",
    "\n",
    "integer_regex = ' Michael Jordan was Born in (\\d)+.'\n",
    "question = 'In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix.'\n",
    "\n",
    "print(\"Without enforcing\")\n",
    "result = run(question, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS)\n",
    "print(result)\n",
    "\n",
    "print(f\"With regex force. Regex: {integer_regex}\")\n",
    "result = run(question, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_regex=integer_regex)\n",
    "print(result)\n",
    "\n",
    "print(\"With string force\")\n",
    "result = run(question, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_str='The answer is 1963')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we demnostrate using ```JsonSchemaParser```. We create a pydantic model, generate the schema from it, and use that to enforce the format.\n",
    "The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With json schema enforcing.\n",
      "   generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
      "0                ▁                29871         1.000000             ▁              29871       1.000000\n",
      "1               ▁{                  426         0.028748         ▁Sure              18585       0.808105\n",
      "2               ▁\"                  376         0.000098        <0x0A>                 13       0.999512\n",
      "3            first                 4102         0.000322         title               3257       0.944336\n",
      "4                _                29918         0.997559             _              29918       0.997559\n",
      "5             name                  978         1.000000          name                978       1.000000\n",
      "6               \":                 1115         0.957031            \":               1115       0.957031\n",
      "7                \"                29908         0.000003            ▁\"                376       0.999023\n",
      "8          Michael                24083         0.987793       Michael              24083       0.987793\n",
      "9               \",                  613         0.988281            \",                613       0.988281\n",
      "10              ▁\"                  376         0.781738            ▁\"                376       0.781738\n",
      "11            last                 4230         0.992676          last               4230       0.992676\n",
      "12               _                29918         1.000000             _              29918       1.000000\n",
      "13            name                  978         1.000000          name                978       1.000000\n",
      "14              \":                 1115         0.994141            \":               1115       0.994141\n",
      "15              \":                 1115         0.000101            ▁\"                376       0.998047\n",
      "16               J                29967         0.938965             J              29967       0.938965\n",
      "17             ord                  536         0.972656           ord                536       0.972656\n",
      "18              an                  273         1.000000            an                273       1.000000\n",
      "19              \",                  613         0.995605            \",                613       0.995605\n",
      "20              ▁\"                  376         0.991699            ▁\"                376       0.991699\n",
      "21            year                 6360         0.998047          year               6360       0.998047\n",
      "22               _                29918         0.999512             _              29918       0.999512\n",
      "23              of                  974         1.000000            of                974       1.000000\n",
      "24               _                29918         0.999512             _              29918       0.999512\n",
      "25               b                29890         1.000000             b              29890       1.000000\n",
      "26            irth                 7515         1.000000          irth               7515       1.000000\n",
      "27              \":                 1115         0.995605            \":               1115       0.995605\n",
      "28               1                29896         0.940430             1              29896       0.940430\n",
      "29               9                29929         1.000000             9              29929       1.000000\n",
      "30               6                29953         0.999512             6              29953       0.999512\n",
      "31               3                29941         0.999512             3              29941       0.999512\n",
      "32               ,                29892         0.968750             ,              29892       0.968750\n",
      "33              ▁\"                  376         0.996094            ▁\"                376       0.996094\n",
      "34             num                 1949         0.998535           num               1949       0.998535\n",
      "35               _                29918         1.000000             _              29918       1.000000\n",
      "36              se                  344         0.999512            se                344       0.999512\n",
      "37           asons                 7040         1.000000         asons               7040       1.000000\n",
      "38               _                29918         0.999512             _              29918       0.999512\n",
      "39              in                  262         0.999512            in                262       0.999512\n",
      "40               _                29918         1.000000             _              29918       1.000000\n",
      "41               n                29876         1.000000             n              29876       1.000000\n",
      "42              ba                 2291         1.000000            ba               2291       1.000000\n",
      "43              \":                 1115         0.999512            \":               1115       0.999512\n",
      "44               1                29896         0.901855             1              29896       0.901855\n",
      "45               5                29945         0.991211             5              29945       0.991211\n",
      "46               }                29913         0.714355             }              29913       0.714355\n",
      "47            </s>                    2         0.055023        <0x0A>                 13       0.783691\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"title\": \"AnswerFormat\", \"type\": \"object\", \"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"]} [/INST]  { \"first_name\":\"Michael\", \"last_name\":\":Jordan\", \"year_of_birth\":1963, \"num_seasons_in_nba\":15}\n",
      "Without json schema enforcing (but with prompt engineering).\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"title\": \"AnswerFormat\", \"type\": \"object\", \"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"]} [/INST]  Sure, I'd be happy to help! Here's the information about Michael Jordan in the format you requested:\n",
      "\n",
      "{\n",
      "\"title\": \"AnswerFormat\",\n",
      "\"type\": \"object\",\n",
      "\"properties\": {\n",
      "\"first_name\": {\n",
      "\"title\": \"First Name\",\n",
      "\"type\": \"string\"\n",
      "},\n",
      "\"last_name\": {\n",
      "\"title\": \"Last Name\",\n",
      "\"type\": \"string\"\n",
      "},\n",
      "\"year_\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "MAX_MAX_NEW_TOKENS = 200\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "MAX_INPUT_TOKEN_LENGTH = 4000\n",
    "floating_point_regex = 'Michael Jordan was Born in (\\d)+(.\\d+)?'\n",
    "integer_regex = ' Michael Jordan was Born in (\\d)+.'\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "\n",
    "print(f\"With json schema enforcing.\")\n",
    "result = run(question_with_schema, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_json_schema=AnswerFormat.schema())\n",
    "print(result)\n",
    "# { \"first_name\":\"Michael\", \"last_name\":\".Jordan\", \"year_of_birth\":1963, \"num_seasons_in_nba\":15}\n",
    "\n",
    "print(f\"Without json schema enforcing (but with prompt engineering).\")\n",
    "result = run(question_with_schema, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS)\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enforced run outputted the following:\n",
    "\n",
    "```json\n",
    "{ \"first_name\":\"Michael\", \"last_name\":\":Jordan\", \"year_of_birth\":1963, \"num_seasons_in_nba\":15}\n",
    "```\n",
    "\n",
    "The unenforced run outputted the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "\"title\": \"AnswerFormat\",\n",
    "\"type\": \"object\",\n",
    "\"properties\": {\n",
    "\"first_name\": {\n",
    "\"title\": \"First Name\",\n",
    "\"type\": \"string\"\n",
    "},\n",
    "\"last_name\": {\n",
    "\"title\": \"Last Name\",\n",
    "\"type\": \"string\"\n",
    "},\n",
    "\"year_\n",
    "```\n",
    "\n",
    "Both runs used the exact same prompt. Token index 3 shows where the enforcer had to be aggressive:\n",
    "\n",
    "\n",
    "```\n",
    "generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
    "\n",
    "3            first                 4102         0.000322         title               3257       0.944336\n",
    "```\n",
    "\n",
    "The language model was trying to generate the word \"title\" (post softmax score of 0.944336) but the enforcer made the \"first\" token (post softmax score of 0.000322) be the main candidate instead.\n",
    "This can be used to further improve the prompt engineering, as we generally want to avoid timesteps that cause the enforcer to be this aggressive, as it increases the likelyhood of hallucinations. For example, The langchain project removes the \"title\" from the json schema, probably for this reason. See here: https://github.com/langchain-ai/langchain/blob/cfa2203c626a2287d60c1febeb3e3a68b77acd77/libs/langchain/langchain/output_parsers/pydantic.py#L40\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmformatenforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
