{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the COLAB runtime (user action required)\n",
    "\n",
    "This colab-friendly notebook is targeted at demoing the enforcer on LLAMA2. It can run on a free GPU on Google Colab.\n",
    "Make sure that your runtime is set to GPU:\n",
    "\n",
    "Menu Bar -> Runtime -> Change runtime type -> T4 GPU (at the time of writing this notebook). [Guide here](https://www.codesansar.com/deep-learning/using-free-gpu-tpu-google-colab.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering huggingface credentials (user action required)\n",
    "\n",
    "We begin by installing the dependencies. This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an access token, and insert it when executing the next cell will request it.\n",
    "\n",
    "Links:\n",
    "\n",
    "- [Request access to llama model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). See the \"Access Llama 2 on Hugging Face\" section.\n",
    "- [Create huggingface access token](https://huggingface.co/settings/tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch lm-format-enforcer huggingface_hub accelerate bitsandbytes cpm_kernels\n",
    "!huggingface-cli login\n",
    "\n",
    "# When running from source / developing the library, use this instead\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamgat/mambaforge/envs/commentranker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:44<00:00, 112.07s/it]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "device = 'cuda'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.pretraining_tp = 1\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    raise Exception('GPU not available')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    # Required for batching example\n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell executed successfully, you have propertly set up your Colab runtime and huggingface account!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the prompt for the specific language model\n",
    "\n",
    "We set up the prompting style according to the demo at https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/app.py . We simplify the implementation a bit as we don't need chat history for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message: str, system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling generate_enforced() instead of model.generate()\n",
    "\n",
    "The main function is fairly straightforward, except for the optional parameters ```required_regex``` / ```required_str``` / ```required_json_schema``` which activate the appropriate ```CharacterLevelParser``` to be used by the format enforcer.\n",
    "\n",
    "The implementation includes ```output_scores=True``` when calling ```generate_enforced()```, which returns diagnostic information about the enforcer's actions. We will use this to understand the results later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import Tuple, Optional, Union, List\n",
    "import pandas as pd\n",
    "from lmformatenforcer import JsonSchemaParser, CharacterLevelParser, RegexParser, StringParser, generate_enforced\n",
    "\n",
    "StringOrManyStrings = Union[str, List[str]]\n",
    "\n",
    "def run(message: StringOrManyStrings,\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 50,\n",
    "        num_beams: int = 1,\n",
    "        required_regex: Optional[str] = None,\n",
    "        required_str: Optional[str] = None,\n",
    "        required_json_schema: Optional[dict] = None) -> Tuple[StringOrManyStrings, Optional[pd.DataFrame]]:\n",
    "    is_multi_message = isinstance(message, list)\n",
    "    messages = message if is_multi_message else [message]\n",
    "    prompts = [get_prompt(msg, system_prompt) for msg in messages]\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', add_special_tokens=False, return_token_type_ids=False, padding=is_multi_message).to(device)\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        inputs,\n",
    "        # streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature,\n",
    "        num_beams=num_beams,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    parser: Optional[CharacterLevelParser] = None\n",
    "    if required_regex:\n",
    "        parser = RegexParser(required_regex)\n",
    "    if required_str:\n",
    "        parser = StringParser(required_str)\n",
    "    if required_json_schema:\n",
    "        parser = JsonSchemaParser(required_json_schema)\n",
    "\n",
    "    if parser:\n",
    "        output = generate_enforced(model, tokenizer, parser, **generate_kwargs)\n",
    "    else:\n",
    "        output = model.generate(**generate_kwargs)\n",
    "\n",
    "    sequences = output['sequences']\n",
    "    # skip_prompt=True doesn't work consistenly, so we hack around it.\n",
    "    string_outputs = [tokenizer.decode(sequence, skip_special_tokens=True) for sequence in sequences]\n",
    "    string_outputs = [string_output.replace(prompt[3:], ' ') for string_output, prompt in zip(string_outputs, prompts)]\n",
    "    if parser and not is_multi_message:\n",
    "        enforced_scores_dict = output.enforced_scores\n",
    "        enforced_scores = pd.DataFrame(enforced_scores_dict)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        pd.set_option('display.max_columns', 10)\n",
    "        pd.set_option('display.max_rows', 999)\n",
    "        pd.set_option('display.float_format', ' {:,.5f}'.format)\n",
    "    else:\n",
    "        enforced_scores = None\n",
    "    return string_outputs if is_multi_message else string_outputs[0], enforced_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Use case\n",
    "\n",
    "Now we demnostrate using ```JsonSchemaParser```. We create a pydantic model, generate the schema from it, and use that to enforce the format.\n",
    "The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"title\": \"AnswerFormat\", \"type\": \"object\", \"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"]}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   { \"first_name\": \"Michael\", \"last_name\": \"Jordan\", \"year_of_birth\": 1963, \"num_seasons_in_nba\": 15 }\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, Without json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   Of course, I'd be happy to provide information about Michael Jordan using the provided JSON schema. Here's the response:\n",
       "{\n",
       "\"title\": \"AnswerFormat\",\n",
       "\"type\": \"object\",\n",
       "\"properties\": {\n",
       "\"first_name\": {\n",
       "\"title\": \"First Name\",\n",
       "\"type\": \"string\",\n",
       "\"required\": true\n",
       "\n",
       "},\n",
       "\"last_name\": {\n",
       "\n",
       "\"title\": \"Last Name\",\n",
       "\n",
       "\"type\":\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from IPython.display import display, Markdown\n",
    "from typing import List\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "\n",
    "display_header(\"Question:\")\n",
    "display_content(question_with_schema)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "result, enforced_scores = run(question_with_schema, system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_json_schema=AnswerFormat.schema())\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result, _ = run(question_with_schema, system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS)\n",
    "display_content(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the results\n",
    "Both runs used the exact same prompt, but only the enforced run produced a valid JSON output.\n",
    "How did the enforcer cause the output to conform to the format? Lets look at the enforcer intervention table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Enforcer intervention table**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_token</th>\n",
       "      <th>generated_token_idx</th>\n",
       "      <th>generated_score</th>\n",
       "      <th>leading_token</th>\n",
       "      <th>leading_token_idx</th>\n",
       "      <th>leading_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁{</td>\n",
       "      <td>426</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>▁Of</td>\n",
       "      <td>4587</td>\n",
       "      <td>0.99382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>0.00312</td>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>0.99674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first</td>\n",
       "      <td>4102</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>title</td>\n",
       "      <td>3257</td>\n",
       "      <td>0.99825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>0.99994</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>0.99994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>name</td>\n",
       "      <td>978</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>name</td>\n",
       "      <td>978</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>0.99986</td>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>0.99986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>0.99994</td>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>0.99994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Michael</td>\n",
       "      <td>24083</td>\n",
       "      <td>0.99161</td>\n",
       "      <td>Michael</td>\n",
       "      <td>24083</td>\n",
       "      <td>0.99161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\",</td>\n",
       "      <td>613</td>\n",
       "      <td>0.98919</td>\n",
       "      <td>\",</td>\n",
       "      <td>613</td>\n",
       "      <td>0.98919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>0.99986</td>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>0.99986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>last</td>\n",
       "      <td>4230</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>last</td>\n",
       "      <td>4230</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>name</td>\n",
       "      <td>978</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>name</td>\n",
       "      <td>978</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>J</td>\n",
       "      <td>29967</td>\n",
       "      <td>0.99981</td>\n",
       "      <td>J</td>\n",
       "      <td>29967</td>\n",
       "      <td>0.99981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ord</td>\n",
       "      <td>536</td>\n",
       "      <td>0.99993</td>\n",
       "      <td>ord</td>\n",
       "      <td>536</td>\n",
       "      <td>0.99993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>an</td>\n",
       "      <td>273</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>an</td>\n",
       "      <td>273</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\",</td>\n",
       "      <td>613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>\",</td>\n",
       "      <td>613</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>year</td>\n",
       "      <td>6360</td>\n",
       "      <td>0.99997</td>\n",
       "      <td>year</td>\n",
       "      <td>6360</td>\n",
       "      <td>0.99997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>of</td>\n",
       "      <td>974</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>of</td>\n",
       "      <td>974</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b</td>\n",
       "      <td>29890</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "      <td>29890</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>irth</td>\n",
       "      <td>7515</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>irth</td>\n",
       "      <td>7515</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9</td>\n",
       "      <td>29929</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9</td>\n",
       "      <td>29929</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6</td>\n",
       "      <td>29953</td>\n",
       "      <td>0.99995</td>\n",
       "      <td>6</td>\n",
       "      <td>29953</td>\n",
       "      <td>0.99995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>0.99983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>,</td>\n",
       "      <td>29892</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>,</td>\n",
       "      <td>29892</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>▁\"</td>\n",
       "      <td>376</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>num</td>\n",
       "      <td>1949</td>\n",
       "      <td>0.99994</td>\n",
       "      <td>num</td>\n",
       "      <td>1949</td>\n",
       "      <td>0.99994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>se</td>\n",
       "      <td>344</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>se</td>\n",
       "      <td>344</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>asons</td>\n",
       "      <td>7040</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>asons</td>\n",
       "      <td>7040</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>in</td>\n",
       "      <td>262</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>in</td>\n",
       "      <td>262</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>_</td>\n",
       "      <td>29918</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>n</td>\n",
       "      <td>29876</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>n</td>\n",
       "      <td>29876</td>\n",
       "      <td>0.99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ba</td>\n",
       "      <td>2291</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>ba</td>\n",
       "      <td>2291</td>\n",
       "      <td>0.99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>\":</td>\n",
       "      <td>1115</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>0.99989</td>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>0.99989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5</td>\n",
       "      <td>29945</td>\n",
       "      <td>0.99345</td>\n",
       "      <td>5</td>\n",
       "      <td>29945</td>\n",
       "      <td>0.99345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>▁}</td>\n",
       "      <td>500</td>\n",
       "      <td>0.99824</td>\n",
       "      <td>▁}</td>\n",
       "      <td>500</td>\n",
       "      <td>0.99824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01614</td>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>0.98331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
       "0                ▁                29871          0.99999             ▁              29871        0.99999\n",
       "1               ▁{                  426          0.00001           ▁Of               4587        0.99382\n",
       "2               ▁\"                  376          0.00312        <0x0A>                 13        0.99674\n",
       "3            first                 4102          0.00022         title               3257        0.99825\n",
       "4                _                29918          0.99994             _              29918        0.99994\n",
       "5             name                  978          1.00000          name                978        1.00000\n",
       "6               \":                 1115          0.99986            \":               1115        0.99986\n",
       "7               ▁\"                  376          0.99994            ▁\"                376        0.99994\n",
       "8          Michael                24083          0.99161       Michael              24083        0.99161\n",
       "9               \",                  613          0.98919            \",                613        0.98919\n",
       "10              ▁\"                  376          0.99986            ▁\"                376        0.99986\n",
       "11            last                 4230          1.00000          last               4230        1.00000\n",
       "12               _                29918          1.00000             _              29918        1.00000\n",
       "13            name                  978          1.00000          name                978        1.00000\n",
       "14              \":                 1115          1.00000            \":               1115        1.00000\n",
       "15              ▁\"                  376          1.00000            ▁\"                376        1.00000\n",
       "16               J                29967          0.99981             J              29967        0.99981\n",
       "17             ord                  536          0.99993           ord                536        0.99993\n",
       "18              an                  273          1.00000            an                273        1.00000\n",
       "19              \",                  613          1.00000            \",                613        1.00000\n",
       "20              ▁\"                  376          1.00000            ▁\"                376        1.00000\n",
       "21            year                 6360          0.99997          year               6360        0.99997\n",
       "22               _                29918          1.00000             _              29918        1.00000\n",
       "23              of                  974          1.00000            of                974        1.00000\n",
       "24               _                29918          1.00000             _              29918        1.00000\n",
       "25               b                29890          1.00000             b              29890        1.00000\n",
       "26            irth                 7515          1.00000          irth               7515        1.00000\n",
       "27              \":                 1115          1.00000            \":               1115        1.00000\n",
       "28               ▁                29871          0.99998             ▁              29871        0.99998\n",
       "29               1                29896          1.00000             1              29896        1.00000\n",
       "30               9                29929          1.00000             9              29929        1.00000\n",
       "31               6                29953          0.99995             6              29953        0.99995\n",
       "32               3                29941          0.99983             3              29941        0.99983\n",
       "33               ,                29892          1.00000             ,              29892        1.00000\n",
       "34              ▁\"                  376          1.00000            ▁\"                376        1.00000\n",
       "35             num                 1949          0.99994           num               1949        0.99994\n",
       "36               _                29918          1.00000             _              29918        1.00000\n",
       "37              se                  344          1.00000            se                344        1.00000\n",
       "38           asons                 7040          1.00000         asons               7040        1.00000\n",
       "39               _                29918          1.00000             _              29918        1.00000\n",
       "40              in                  262          1.00000            in                262        1.00000\n",
       "41               _                29918          1.00000             _              29918        1.00000\n",
       "42               n                29876          0.99999             n              29876        0.99999\n",
       "43              ba                 2291          0.99999            ba               2291        0.99999\n",
       "44              \":                 1115          1.00000            \":               1115        1.00000\n",
       "45               ▁                29871          1.00000             ▁              29871        1.00000\n",
       "46               1                29896          0.99989             1              29896        0.99989\n",
       "47               5                29945          0.99345             5              29945        0.99345\n",
       "48              ▁}                  500          0.99824            ▁}                500        0.99824\n",
       "49            </s>                    2          0.01614        <0x0A>                 13        0.98331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_header(\"Enforcer intervention table\")\n",
    "display(enforced_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token index 3 shows where the enforcer had to be aggressive:\n",
    "\n",
    "\n",
    "```\n",
    "generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
    "\n",
    "3            first                 4102         0.00032         title               3257       0.94433\n",
    "```\n",
    "\n",
    "The language model was trying to generate the word \"title\" (post softmax score of 0.94433) but the enforcer made the \"first\" token (post softmax score of 0.00032) be the main candidate instead.\n",
    "This can be used to further improve the prompt engineering, as we generally want to avoid timesteps that cause the enforcer to be this aggressive, as it increases the likelyhood of hallucinations. For example, The [langchain project removes the \"title\" from the json schema](https://github.com/langchain-ai/langchain/blob/cfa2203c626a2287d60c1febeb3e3a68b77acd77/libs/langchain/langchain/output_parsers/pydantic.py#L40), probably for this reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-JSON Use Cases\n",
    "Here are a few examples for the simple (non-json) case. Note that the ```required_regex``` does not support the full regex syntax, see the README for full limiations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Without enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   Thank you for your question! Michael Jordan was born in the year 1963.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**With regex force. Regex:  Michael Jordan was born in (\\d)+.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Language model output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  Michael Jordan was born in 1963.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Enforcer intervention table:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_token</th>\n",
       "      <th>generated_token_idx</th>\n",
       "      <th>generated_score</th>\n",
       "      <th>leading_token</th>\n",
       "      <th>leading_token_idx</th>\n",
       "      <th>leading_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>24083</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>▁Thank</td>\n",
       "      <td>3374</td>\n",
       "      <td>0.60538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁Jordan</td>\n",
       "      <td>18284</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>▁Jordan</td>\n",
       "      <td>18284</td>\n",
       "      <td>0.99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁was</td>\n",
       "      <td>471</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>▁was</td>\n",
       "      <td>471</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁born</td>\n",
       "      <td>6345</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>▁born</td>\n",
       "      <td>6345</td>\n",
       "      <td>0.99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁in</td>\n",
       "      <td>297</td>\n",
       "      <td>0.99978</td>\n",
       "      <td>▁in</td>\n",
       "      <td>297</td>\n",
       "      <td>0.99978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.29098</td>\n",
       "      <td>▁the</td>\n",
       "      <td>278</td>\n",
       "      <td>0.70902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>29929</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9</td>\n",
       "      <td>29929</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>29953</td>\n",
       "      <td>0.99998</td>\n",
       "      <td>6</td>\n",
       "      <td>29953</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>0.99983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>29889</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>.</td>\n",
       "      <td>29889</td>\n",
       "      <td>0.99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0.98431</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0.98431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
       "0                ▁                29871          0.99998             ▁              29871        0.99998\n",
       "1          Michael                24083          0.00011        ▁Thank               3374        0.60538\n",
       "2          ▁Jordan                18284          0.99999       ▁Jordan              18284        0.99999\n",
       "3             ▁was                  471          0.99998          ▁was                471        0.99998\n",
       "4            ▁born                 6345          0.99999         ▁born               6345        0.99999\n",
       "5              ▁in                  297          0.99978           ▁in                297        0.99978\n",
       "6                ▁                29871          0.29098          ▁the                278        0.70902\n",
       "7                1                29896          1.00000             1              29896        1.00000\n",
       "8                9                29929          1.00000             9              29929        1.00000\n",
       "9                6                29953          0.99998             6              29953        0.99998\n",
       "10               3                29941          0.99983             3              29941        0.99983\n",
       "11               .                29889          0.99999             .              29889        0.99999\n",
       "12            </s>                    2          0.98431          </s>                  2        0.98431"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**With string force:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Language model output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       " The answer is 1963\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Enforcer intervention table:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_token</th>\n",
       "      <th>generated_token_idx</th>\n",
       "      <th>generated_score</th>\n",
       "      <th>leading_token</th>\n",
       "      <th>leading_token_idx</th>\n",
       "      <th>leading_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>1576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁answer</td>\n",
       "      <td>1234</td>\n",
       "      <td>0.23457</td>\n",
       "      <td>▁question</td>\n",
       "      <td>1139</td>\n",
       "      <td>0.26999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁is</td>\n",
       "      <td>338</td>\n",
       "      <td>0.03131</td>\n",
       "      <td>▁to</td>\n",
       "      <td>304</td>\n",
       "      <td>0.92951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.59562</td>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>0.59562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>0.97437</td>\n",
       "      <td>1</td>\n",
       "      <td>29896</td>\n",
       "      <td>0.97437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>29929</td>\n",
       "      <td>0.99911</td>\n",
       "      <td>9</td>\n",
       "      <td>29929</td>\n",
       "      <td>0.99911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>29953</td>\n",
       "      <td>0.99843</td>\n",
       "      <td>6</td>\n",
       "      <td>29953</td>\n",
       "      <td>0.99843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>0.99771</td>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>0.99771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>.</td>\n",
       "      <td>29889</td>\n",
       "      <td>0.98834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  generated_token  generated_token_idx  generated_score leading_token  leading_token_idx  leading_score\n",
       "0             The                 1576          0.00000             ▁              29871        0.99998\n",
       "1         ▁answer                 1234          0.23457     ▁question               1139        0.26999\n",
       "2             ▁is                  338          0.03131           ▁to                304        0.92951\n",
       "3               ▁                29871          0.59562             ▁              29871        0.59562\n",
       "4               1                29896          0.97437             1              29896        0.97437\n",
       "5               9                29929          0.99911             9              29929        0.99911\n",
       "6               6                29953          0.99843             6              29953        0.99843\n",
       "7               3                29941          0.99771             3              29941        0.99771\n",
       "8            </s>                    2          0.00544             .              29889        0.98834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "MAX_MAX_NEW_TOKENS = 200\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "MAX_INPUT_TOKEN_LENGTH = 4000\n",
    "\n",
    "integer_regex = ' Michael Jordan was born in (\\d)+.'\n",
    "question = 'In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix.'\n",
    "display_header(\"Question:\")\n",
    "display_content(question)\n",
    "\n",
    "display_header(\"Without enforcing:\")\n",
    "result, _ = run(question, system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS)\n",
    "display_content(result)\n",
    "\n",
    "print('\\n----------------------------------------\\n')\n",
    "\n",
    "display_header(f\"With regex force. Regex: {integer_regex}\")\n",
    "result, enforced_scores = run(question, system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_regex=integer_regex)\n",
    "display_header(\"Language model output:\")\n",
    "display_content(result)\n",
    "display_header(\"Enforcer intervention table:\")\n",
    "display(enforced_scores)\n",
    "\n",
    "print('\\n----------------------------------------\\n')\n",
    "\n",
    "display_header(\"With string force:\")\n",
    "result, enforced_scores = run(question, system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_str='The answer is 1963')\n",
    "display_header(\"Language model output:\")\n",
    "display_content(result)\n",
    "display_header(\"Enforcer intervention table:\")\n",
    "display(enforced_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching example\n",
    "\n",
    "This is a simple example of using batching to generate multiple queries in parallel. All outputs will be in the correct format. Every timestep can filter different tokens for the different batch indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Batched Answers, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   { \"first_name\": \"Michael\", \"last_name\": \"Jordan\", \"year_of_birth\": 1963, \"num_seasons_in_nba\": 15 }\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   { \"first_name\": \"Timothy\", \"last_name\": \"Duncan\", \"year_of_birth\": 1976, \"num_seasons_in_nba\": 19 }\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   { \"first_name\": \"Kobe\", \"last_name\": \"Bryant\", \"year_of_birth\": 1978, \"num_seasons_in_nba\": 20 }\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "   { \"first_name\": \"Kareem\", \"last_name\": \"Abdul-Jabbar\", \"year_of_birth\": 1947, \"num_seasons_in_nba\": 20 }\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PLAYER_NAMES = ['Michael Jordan', 'Tim Duncan', 'Kobe Bryant', 'Kareem Abdul Jabbar']\n",
    "question = 'Please give me information about {0}. You MUST answer using the following json schema: '\n",
    "questions_with_schema = [f'{question.format(player_name)}{AnswerFormat.schema_json()}' for player_name in PLAYER_NAMES]\n",
    "\n",
    "display_header(\"Batched Answers, With json schema enforcing:\")\n",
    "results, _ = run(questions_with_schema, system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_json_schema=AnswerFormat.schema())\n",
    "for result in results:\n",
    "    display_content(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmformatenforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
