{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/tree/main\n",
    "from charset_normalizer.utils import is_punctuation, is_symbol, unicode_range, is_accentuated, is_latin, \\\n",
    "    remove_accent, is_separator, is_cjk, is_case_variable, is_hangul, is_katakana, is_hiragana, is_ascii, is_thai\n",
    "\n",
    "from threading import Thread\n",
    "from typing import Iterator, List, Set\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "from regexparser import SequentialPattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "device = 'cuda'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.pretraining_tp = 1\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    model = None\n",
    "fast_tokenizer = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=fast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message: str, chat_history: list[tuple[str, str]],\n",
    "               system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    for user_input, response in chat_history:\n",
    "        user_input = user_input.strip() if do_strip else user_input\n",
    "        do_strip = True\n",
    "        texts.append(f'{user_input} [/INST] {response.strip()} </s><s>[INST] ')\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)\n",
    "\n",
    "\n",
    "def get_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> int:\n",
    "    prompt = get_prompt(message, chat_history, system_prompt)\n",
    "    input_ids = tokenizer([prompt], return_tensors='np', add_special_tokens=False)['input_ids']\n",
    "    return input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from lmformatenforcer import JsonSchemaParser, CharacterLevelParser, RegexParser, StringParser, generate_enforced\n",
    "\n",
    "\n",
    "def run(message: str,\n",
    "        chat_history: list[tuple[str, str]],\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 50,\n",
    "        required_regex: str = None,\n",
    "        required_str: str = None,\n",
    "        required_json_schema: dict = None) -> str:\n",
    "    prompt = get_prompt(message, chat_history, system_prompt)\n",
    "    inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False, return_token_type_ids=False).to(device)\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        inputs,\n",
    "        # streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature,\n",
    "        num_beams=1,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    parser: CharacterLevelParser = None\n",
    "    if required_regex:\n",
    "        parser = RegexParser(required_regex)\n",
    "    if required_str:\n",
    "        parser = StringParser(required_str)\n",
    "    if required_json_schema:\n",
    "        parser = JsonSchemaParser(required_json_schema)\n",
    "\n",
    "    if parser:\n",
    "        output = generate_enforced(model, tokenizer, parser, **generate_kwargs)\n",
    "    else:\n",
    "        output = model.generate(**generate_kwargs)\n",
    "\n",
    "    sequence = output.sequences[0]\n",
    "    string_output = tokenizer.decode(sequence, skip_special_tokens=True, skip_prompt=True)\n",
    "    enforced_scores = output.enforced_scores\n",
    "    \n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_columns', 10)\n",
    "    pd.set_option('display.max_rows', 200)\n",
    "    print(enforced_scores)\n",
    "    return string_output\n",
    "    \n",
    "    #t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    #t.start()\n",
    "\n",
    "    # outputs = []\n",
    "    # for text in streamer:\n",
    "    #     outputs.append(text)\n",
    "    #   yield ''.join(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "MAX_MAX_NEW_TOKENS = 200\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "MAX_INPUT_TOKEN_LENGTH = 4000\n",
    "floating_point_regex = 'Michael Jordan was Born in (\\d)+(.\\d+)?'\n",
    "integer_regex = ' Michael Jordan was Born in (\\d)+.'\n",
    "question = 'In what year was Michael Jordan Born? Please answer using only a number, without any prefix or suffix.'\n",
    "\n",
    "# print(\"Without enforcing\")\n",
    "# result = run(question, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS)\n",
    "# print(result)\n",
    "\n",
    "print(f\"With regex force. Regex: {integer_regex}\")\n",
    "result = run(question, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_regex=integer_regex)\n",
    "print(result)\n",
    "\n",
    "# print(\"With string force\")\n",
    "# result = run(question, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_str='The answer is 1963')\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "MAX_MAX_NEW_TOKENS = 200\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "MAX_INPUT_TOKEN_LENGTH = 4000\n",
    "floating_point_regex = 'Michael Jordan was Born in (\\d)+(.\\d+)?'\n",
    "integer_regex = ' Michael Jordan was Born in (\\d)+.'\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "\n",
    "print(f\"With json schema force force.\")\n",
    "result = run(question_with_schema, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, required_json_schema=AnswerFormat.schema())\n",
    "print(result)\n",
    "\n",
    "# abcdef\n",
    "# michael\n",
    "# and\n",
    "# ing\n",
    "# gratify\n",
    "# ing\n",
    "# print(f\"Without json schema force force.\")\n",
    "# result = run(question_with_schema, chat_history=[], system_prompt=DEFAULT_SYSTEM_PROMPT, max_new_tokens=DEFAULT_MAX_NEW_TOKENS)\n",
    "# print(result)\n",
    "# \"first_name\":\"Michael\", \"last_name\"\n",
    "\n",
    "# { \"first_name\":\"Michael\", \"last_name\":\".Jordan\", \"year_of_birth\":1963, \"num_seasons_in_nba\":15}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commentranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
