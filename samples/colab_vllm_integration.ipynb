{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Format Enforcer Integration with vLLM\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_vllm_integration.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook shows how you can integrate with the vLLM library. vLLM does not currently have an API for token filtering, so we have to do some monkey patching to expose the functionality.\n",
    "\n",
    "## Setting up the COLAB runtime (user action required)\n",
    "\n",
    "This colab-friendly notebook is targeted at demoing the enforcer on LLAMA2. It can run on a free GPU on Google Colab.\n",
    "Make sure that your runtime is set to GPU:\n",
    "\n",
    "Menu Bar -> Runtime -> Change runtime type -> T4 GPU (at the time of writing this notebook). [Guide here](https://www.codesansar.com/deep-learning/using-free-gpu-tpu-google-colab.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering huggingface credentials (user action required)\n",
    "\n",
    "We begin by installing the dependencies. This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an access token, and insert it when executing the next cell will request it.\n",
    "\n",
    "Links:\n",
    "\n",
    "- [Request access to llama model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). See the \"Access Llama 2 on Hugging Face\" section.\n",
    "- [Create huggingface access token](https://huggingface.co/settings/tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm lm-format-enforcer\n",
    "!huggingface-cli login\n",
    "\n",
    "# When running from source / developing the library, use this instead\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "## os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom sampler that filters tokens\n",
    "\n",
    "We introduce a subclass of vLLM's ```SamplingParams``` that also accepts a token filtering function, with the same API as Huggingface Transformers \n",
    "\n",
    "```prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]]```\n",
    "\n",
    "We then introduce the function ```_apply_allowed_token_filters()``` that applies the filter functions to the logits (sets them to negative infinity if not allowed) to requests that contain a filter function.\n",
    "\n",
    "We hope that in future releases of vLLM, this (or similar) will be part of vLLM's ```Sampler``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamgat/mambaforge/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-01 00:01:40,977\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-11-01 00:01:41,234\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from vllm.model_executor.input_metadata import InputMetadata\n",
    "\n",
    "class SamplingParamsWithFilterFunction(SamplingParams):\n",
    "    logits_allowed_tokens_filter_function: Optional[Callable[[int, torch.Tensor], List[int]]]\n",
    "\n",
    "def _apply_allowed_token_filters(logits: torch.Tensor, \n",
    "                                 input_metadata: InputMetadata) -> torch.Tensor:\n",
    "    num_seqs, vocab_size = logits.shape\n",
    "    logits_row_idx = 0\n",
    "    for seq_ids, sampling_params in input_metadata.seq_groups:\n",
    "        if isinstance(sampling_params, SamplingParamsWithFilterFunction):\n",
    "            filter_function = sampling_params.logits_allowed_tokens_filter_function\n",
    "        else:\n",
    "            filter_function = None\n",
    "        for seq_id in seq_ids:\n",
    "            if filter_function is not None:\n",
    "                output_token_ids = input_metadata.seq_data[seq_id].output_token_ids\n",
    "                output_token_tensor = torch.tensor(output_token_ids, dtype=torch.long)\n",
    "                allowed_tokens = filter_function(logits_row_idx, output_token_tensor)\n",
    "                logits_add_factor = torch.zeros(vocab_size, dtype=logits.dtype, device=logits.device)\n",
    "                logits_add_factor[:] = float('-inf')\n",
    "                logits_add_factor[allowed_tokens] = 0\n",
    "                logits[logits_row_idx] += logits_add_factor\n",
    "            logits_row_idx += 1\n",
    "    assert logits_row_idx == num_seqs\n",
    "    return logits \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integrate this function with the ```Sampler``` class, we have to change its ```forward()``` function to call it. Since we are not modifying vLLM itself, we will do this with monkey patching. \n",
    "\n",
    "Other than the line \n",
    "```\n",
    "logits = _apply_allowed_token_filters(logits, input_metadata)\n",
    "```\n",
    "this is a 100% copy of the original ```Sampler.forward()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.model_executor.layers.sampler import SamplerOutput, _prune_hidden_states, _get_logits, _get_output_tokens, _get_penalties, _apply_penalties, _get_temperatures, _get_top_p_top_k, _apply_top_p_top_k, _sample, _get_logprobs, _build_sampler_output, _SAMPLING_EPS\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "def patched_forward(\n",
    "        self,\n",
    "        embedding: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_metadata: InputMetadata,\n",
    "        embedding_bias: Optional[torch.Tensor] = None,\n",
    "    ) -> SamplerOutput:\n",
    "        # Get the hidden states that we use for sampling.\n",
    "        hidden_states = _prune_hidden_states(hidden_states, input_metadata)\n",
    "\n",
    "        # Get the logits for the next tokens.\n",
    "        logits = _get_logits(hidden_states, embedding, embedding_bias,\n",
    "                             self.vocab_size)\n",
    "        \n",
    "        # Apply presence and frequency penalties.\n",
    "        output_tokens = _get_output_tokens(input_metadata)\n",
    "        assert len(output_tokens) == logits.shape[0]\n",
    "        presence_penalties, frequency_penalties = _get_penalties(\n",
    "            input_metadata)\n",
    "        assert len(presence_penalties) == logits.shape[0]\n",
    "        assert len(frequency_penalties) == logits.shape[0]\n",
    "        logits = _apply_penalties(logits, output_tokens, presence_penalties,\n",
    "                                  frequency_penalties)\n",
    "        \n",
    "        ### LM FORMAT ENFORCER MONKEY PATCH START\n",
    "        logits = _apply_allowed_token_filters(logits, input_metadata)\n",
    "        ### LM FORMAT ENFORCER MONKEY PATCH END\n",
    "\n",
    "        # Apply temperature scaling.\n",
    "        temperatures = _get_temperatures(input_metadata)\n",
    "        assert len(temperatures) == logits.shape[0]\n",
    "        if any(t != 1.0 for t in temperatures):\n",
    "            t = torch.tensor(temperatures,\n",
    "                             dtype=logits.dtype,\n",
    "                             device=logits.device)\n",
    "            # Use in-place division to avoid creating a new tensor.\n",
    "            logits.div_(t.unsqueeze(dim=1))\n",
    "\n",
    "        # Apply top-p and top-k truncation.\n",
    "        top_ps, top_ks = _get_top_p_top_k(input_metadata, self.vocab_size)\n",
    "        assert len(top_ps) == len(top_ks) == logits.shape[0]\n",
    "        do_top_p = any(p < 1.0 - _SAMPLING_EPS for p in top_ps)\n",
    "        do_top_k = any(k != self.vocab_size for k in top_ks)\n",
    "        if do_top_p or do_top_k:\n",
    "            logits = _apply_top_p_top_k(logits, top_ps, top_ks)\n",
    "\n",
    "        # We use float32 for probabilities and log probabilities.\n",
    "        # Compute the probabilities.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n",
    "        # Compute the log probabilities.\n",
    "        # Use log_softmax to ensure numerical stability.\n",
    "        logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\n",
    "\n",
    "        # Sample the next tokens.\n",
    "        sample_results = _sample(probs, logprobs, input_metadata)\n",
    "        # Get the logprobs query results.\n",
    "        prompt_logprobs, sample_logprobs = _get_logprobs(\n",
    "            logprobs, input_metadata, sample_results)\n",
    "        return _build_sampler_output(sample_results, input_metadata,\n",
    "                                     prompt_logprobs, sample_logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model, as is normally done with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 00:01:42 llm_engine.py:72] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-chat-hf', tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 11-01 00:01:42 tokenizer.py:31] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "INFO 11-01 00:01:47 llm_engine.py:207] # GPU blocks: 1091, # CPU blocks: 512\n",
      "WARNING 11-01 00:01:47 cache_engine.py:96] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "llm = vllm.LLM(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell executed successfully, you have propertly set up your Colab runtime and huggingface account!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions to make display nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the prompt for the specific language model\n",
    "\n",
    "We set up the prompting style according to the [Llama2 demo](https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/app.py). We simplify the implementation a bit as we don't need chat history for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating the monkey patch and creating the generation function\n",
    "\n",
    "We monkey-patch the ```Sampler``` class with our custom ```forward()``` method, using ```unittest.mock```.\n",
    "\n",
    "We use our sampling params in order to sent the specific filter function with the request. Different requests can have different format enforcers.\n",
    "\n",
    "There is an advanced loop for batch mode, it is done because for some reason we get better performance with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmformatenforcer import CharacterLevelParser\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "from unittest import mock\n",
    "from typing import Union, List\n",
    "from vllm import RequestOutput\n",
    "\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "\n",
    "ListOrStrList = Union[str, List[str]]\n",
    "\n",
    "def vllm_with_character_level_parser(llm: vllm.LLM, prompt: ListOrStrList, parser: Optional[CharacterLevelParser] = None) -> ListOrStrList:\n",
    "    with mock.patch.object(vllm.model_executor.layers.sampler.Sampler, 'forward', patched_forward):\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(llm.get_tokenizer(), parser) if parser else None\n",
    "        sampling_params = SamplingParamsWithFilterFunction()\n",
    "        sampling_params.max_tokens = DEFAULT_MAX_NEW_TOKENS\n",
    "        sampling_params.logits_allowed_tokens_filter_function = prefix_function\n",
    "        if isinstance(prompt, str):\n",
    "            result = llm.generate(prompt, sampling_params=sampling_params)\n",
    "            return result[0].outputs[0].text\n",
    "        else:\n",
    "            # This code works, but for some reason it gives slower generation time.\n",
    "            # results = llm.generate(prompt, sampling_params=sampling_params)\n",
    "            # return [result.outputs[0].text for result in results]\n",
    "            \n",
    "            # Batch mode, taking inspiration from https://github.com/vllm-project/vllm/blob/main/examples/llm_engine_example.py\n",
    "            # I don't know why this is faster than simply calling llm.generate() with a list of prompts, but it is from my tests.\n",
    "            prompts: List[str] = prompt\n",
    "            \n",
    "            engine = llm.llm_engine\n",
    "            request_id = 0\n",
    "            results = []\n",
    "            while prompts or engine.has_unfinished_requests():\n",
    "                if prompts:\n",
    "                    prompt = prompts.pop(0)\n",
    "                    engine.add_request(str(request_id), prompt, sampling_params)\n",
    "                    request_id += 1\n",
    "\n",
    "                request_outputs: List[RequestOutput] = engine.step()\n",
    "\n",
    "                for request_output in request_outputs:\n",
    "                    if request_output.finished:\n",
    "                        results.append(request_output.outputs[0].text)\n",
    "\n",
    "            return results\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM + JSON Use case\n",
    "\n",
    "Now we demonstrate using ```JsonSchemaParser```. We create a pydantic model, generate the schema from it, and use that to enforce the format.\n",
    "The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "<s>[INST] <<SYS>>\n",
       "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
       "\n",
       "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
       "<</SYS>>\n",
       "\n",
       "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"title\": \"AnswerFormat\", \"type\": \"object\", \"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"]} [/INST]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  {\n",
       "\"first_name\": \"Michael\",\n",
       "\"last_name\": \"Jordan\",\n",
       "\"year_of_birth\": 1963,\n",
       "\"num_seasons_in_nba\": 15\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, Without json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  Of course! Here's the requested information about Michael Jordan in the format specified:\n",
       "\n",
       "{\n",
       "\"title\": \"AnswerFormat\",\n",
       "\"type\": \"object\",\n",
       "\"properties\": {\n",
       "\"first_name\": {\n",
       "\"title\": \"First Name\",\n",
       "\"type\": \"string\",\n",
       "\"example\": \"Michael\"\n",
       "},\n",
       "\"last_name\": {\n",
       "\"title\": \"Last Name\",\n",
       "\"type\": \"string\",\n",
       "\"example\": \"J\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "prompt = get_prompt(question_with_schema)\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "\n",
    "result = vllm_with_character_level_parser(llm, prompt, JsonSchemaParser(AnswerFormat.schema()))\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result = vllm_with_character_level_parser(llm, prompt, None)\n",
    "display_content(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the enforced output matches the required schema, while the unenforced does not. We have successfully integrated with vLLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching example\n",
    "\n",
    "Now we demonstrate that the model can be used to generate text in batches. This is useful for generating text in parallel, which is much faster than generating text sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 player: 1.8789570331573486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  {\n",
       "\"first_name\": \"Michael\",\n",
       "\"last_name\": \"Jordan\",\n",
       "\"year_of_birth\": 1963,\n",
       "\"num_seasons_in_nba\": 15\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 19 players: 6.5615339279174805. Time per player: 0.3453438909430253\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "['  {\\n\"first_name\": \"Moses\",\\n\"last_name\": \"Malone\",\\n\"year_of_birth\": 1963,\\n\"num_seasons_in_nba\": 21\\n}', '  {\\n\"first_name\": \"Charles\",\\n\"last_name\": \"Barkley\",\\n\"year_of_birth\": 1963,\\n\"num_seasons_in_nba\": 16\\n}', '  {\\n\"first_name\": \"Dolph\",\\n\"last_name\": \"Schayes\",\\n\"year_of_birth\": 1921,\\n\"num_seasons_in_nba\": 15\\n}', '  {\\n\"first_name\": \"Julius\",\\n\"last_name\": \"Erving\",\\n\"year_of_birth\": 1952,\\n\"num_seasons_in_nba\": 11\\n}\\n\\n', '  {\\n\"first_name\": \"Jerry\",\\n\"last_name\": \"Lucas\",\\n\"year_of_birth\": 1944,\\n\"num_seasons_in_nba\": 12\\n}', '  {\\n\"first_name\": \"Bob\",\\n\"last_name\": \"Pettit\",\\n\"year_of_birth\": 1922,\\n\"num_seasons_in_nba\": 11\\n}\\n\\n', '  {\\n\"first_name\": \"Bill\",\\n\"last_name\": \"Russell\",\\n\"year_of_birth\": 1934,\\n\"num_seasons_in_nba\": 13\\n}\\n\\n', '  {\\n\"first_name\": \"Larry\",\\n\"last_name\": \" Bird\",\\n\"year_of_birth\": 1956,\\n\"num_seasons_in_nba\": 13\\n}\\n\\n\\n\\n', '  {\\n\"first_name\": \"Earvin\",\\n\"last_name\": \"Johnson\",\\n\"year_of_birth\": 1959,\\n\"num_seasons_in_nba\": 13\\n}\\n\\n', '  {\\n\"first_name\": \"Hakeem\",\\n\"last_name\": \"Olajuwon\",\\n\"year_of_birth\": 1963,\\n\"num_seasons_in_nba\": 18\\n}', '  {\\n\"first_name\": \"Clyde\",\\n\"last_name\": \"Drexler\",\\n\"year_of_birth\": 1962,\\n\"num_seasons_in_nba\": 10\\n}', '  {\\n\"first_name\": \"Bob\",\\n\"last_name\": \"Cousy\",\\n\"year_of_birth\": 1928,\\n\"num_seasons_in_nba\": 13\\n}\\n\\n\\n\\n', '  {\\n\"first_name\": \"George\",\\n\"last_name\": \"Mikan\",\\n\"year_of_birth\": 1924,\\n\"num_seasons_in_nba\": 10\\n}\\n\\n\\n\\n', '  {\\n\"first_name\": \"Timothy\",\\n\"last_name\": \"Duncan\",\\n\"year_of_birth\": 1976,\\n\"num_seasons_in_nba\": 19\\n}\\n\\n\\n\\n', '  {\\n\"first_name\": \"John\",\\n\"last_name\": \"Havlicek\",\\n\"year_of_birth\": 1949,\\n\"num_seasons_in_nba\": 16\\n}\\n\\n\\n\\n', '  {\\n\"first_name\": \"Elvin\",\\n\"last_name\": \"Hayes\",\\n\"year_of_birth\": 1945,\\n\"num_seasons_in_nba\": 10\\n}\\n\\n\\n\\n', '  {\\n\"first_name\": \"Patrick\",\\n\"last_name\": \"Ewing\",\\n\"year_of_birth\": 1962,\\n\"num_seasons_in_nba\": 17\\n}\\n\\n\\n\\n\\n', '  {\\n\"first_name\": \"Nate\",\\n\"last_name\": \"Archibald\",\\n\"year_of_birth\": 1953,\\n\"num_seasons_in_nba\": 11\\n}\\n\\n\\n\\n\\n', '  {\\n\"first_name\": \"Oscar\",\\n\"last_name\": \"Robertson\",\\n\"year_of_birth\": 1938,\\n\"num_seasons_in_nba\": 13\\n}\\n\\n\\n\\n\\n']\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "players = ['Michael Jordan', 'Tim Duncan', 'Larry Bird', 'Magic Johnson', 'Patrick Ewing', \n",
    "           'Hakeem Olajuwan', 'Nate Archibald', 'Charles Barkley', 'Bob Cousy', 'Clyde Drexler', \n",
    "           'Julius Erving', 'John Havlicek', 'Elvin Hayes', 'Jerry Lucas', 'Moses Malone',\n",
    "           'George Mikan', 'Bob Pettit', 'Oscar Robertson', 'Bill Russell', 'Dolph Schayes']\n",
    "prompts = []\n",
    "for player in players:\n",
    "    question = f'Please give me information about {player}. You MUST answer using the following json schema: '\n",
    "    question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "    prompt = get_prompt(question_with_schema)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "start = time()\n",
    "one_player_result = vllm_with_character_level_parser(llm, prompts[0], JsonSchemaParser(AnswerFormat.schema()))\n",
    "end = time()\n",
    "print(f'Time taken for 1 player: {end - start}s')\n",
    "display_content(one_player_result)\n",
    "\n",
    "start = time()\n",
    "all_results = vllm_with_character_level_parser(llm, prompts[1:], JsonSchemaParser(AnswerFormat.schema()))\n",
    "end = time()\n",
    "print(f'Time taken for {len(prompts)-1} players: {end - start}. Time per player: {(end - start)/(len(prompts)-1)}')\n",
    "display_content(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmformatenforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
